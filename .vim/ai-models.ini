[default]
# Default is local Ollama with OpenAI compatibility:
options.token_file_path = ~/.vim/ai-token.local
options.endpoint_url = http://127.0.0.1:11434/v1/chat/completions
# These models are acceptably fast even on non-GPU notebooks with enough (16GB+) RAM:
#options.model = gemma3:latest
#options.model = qwen2.5-coder:3b
options.model = qwen2.5-coder:7b

[r]
# OpenRouter.AI
# Check your spending at: https://openrouter.ai/activity
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
# Let's select a working free one by default, as of 2025-04-05
#options.model = google/gemini-2.5-pro-exp-03-25:free
#options.model = microsoft/phi-3-medium-128k-instruct:free
options.model = qwen/qwen2.5-vl-72b-instruct:free

######################
# Pre-defined prompts
######################

[ai]
prompt = Return the same code, but complete the task found in the comment ending with `AI!`.

[fix]
prompt = Return the same code, but fix errors, if any. Mark your changes with `FIXED:` and add a short explanation of the bug or fix.

###############################
# Cheap but good at OpenRouter
###############################

[qwen]
# $0.16
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = qwen/qwen-2.5-coder-32b-instruct

[phi]
# $0.16-$0.3
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = microsoft/phi-4

[gemma]
# $0.2
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = google/gemma-3-27b-it

[mistral]
# $0.25
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = mistralai/codestral-mamba

[llama]
# $0.3-$0.4
#options.model = meta-llama/llama-3.1-70b-instruct
# $0.3-$0.4
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = meta-llama/llama-3.3-70b-instruct

[gemini]
# $0.4
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = google/gemini-2.0-flash-001

[o3]
# $0.44
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = openai/o3-mini-high

[4o]
# $0.6
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = openai/gpt-4o-mini

##########################
# Mid-range at OpenRouter
##########################

[deepseek]
# $2.5-$5
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = deepseek/deepseek-r1

[claude]
# $4
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = anthropic/claude-3.5-haiku

[amazon]
# $3.2
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = amazon/nova-pro-v1

##########################
# Expensive at OpenRouter
##########################

[claude-big]
# $15
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = anthropic/claude-3.7-sonnet

[openai-big]
# $15
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = openai/chatgpt-4o-latest
