[default]
# Default is local Ollama with OpenAI compatibility:
options.token_file_path = ~/.vim/ai-token.local
options.endpoint_url = http://127.0.0.1:11434/v1/chat/completions
# These models are acceptably fast even on non-GPU notebooks with enough (16GB+) RAM:
#options.model = gemma3:latest
#options.model = qwen2.5-coder:3b
#options.model = qwen2.5-coder:7b
options.model = granite3.3:8b

[r]
# OpenRouter.AI
# Check your spending at: https://openrouter.ai/activity
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
# Let's select a working free one by default, as of 2025-05-27
options.model = meta-llama/llama-4-maverick:free
#options.model = meta-llama/llama-4-scout:free
#options.model = qwen/qwen2.5-vl-32b-instruct:free
#options.model = qwen/qwen2.5-vl-72b-instruct:free

######################
# Pre-defined prompts
######################

[ai]
prompt = Return the same code, but complete the task found in the comment ending with `AI!`.

[fix]
prompt = Return the same code, but fix errors, if any. Mark your changes with `FIXED:` and add a short explanation of the bug or fix.

###############################
# Cheap but good at OpenRouter
###############################

[qwen]
# USD 0.16
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = qwen/qwen-2.5-coder-32b-instruct

[gemma]
# USD 0.2
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = google/gemma-3-27b-it

[gemini]
# USD 0.4
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = google/gemini-2.0-flash-001

[4o]
# USD 0.6
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = openai/gpt-4o-mini

[llama4]
# USD 0.6
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = meta-llama/llama-4-maverick

[deepseek]
# USD 1.5
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = deepseek/deepseek-chat-v3-0324

##########################
# Mid-range at OpenRouter
##########################

[claude]
# USD 4
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = anthropic/claude-3.5-haiku

[o3]
# USD 4.4
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = openai/o3-mini-high

[o4]
# USD 4.4
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = openai/o4-mini-high

##########################
# Expensive at OpenRouter
##########################

[gemini-big]
# USD 10-15 (depends on context size)
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = google/gemini-2.5-pro-preview

[claude-big]
# USD 15
options.token_file_path = ~/.vim/ai-token
options.endpoint_url = https://openrouter.ai/api/v1/chat/completions
options.model = anthropic/claude-3.7-sonnet
